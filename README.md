# openAIReinforcementLearning

# CS 238 Proposal
Pujun Bhatnagar, Christopher Heung, Akihiro Matsukawa November 1, 2016

## Problem Description

 OpenAI Gym is a dev tool that provides a diverse suite of environments that range from easy to diﬃcultandinvolvemanydiﬀerentkindsofdata. Ourgoalforthisprojectistoapplyreinforcement learning, speciﬁcally transfer learning, onto the environments given in OpenAI Gym.
## Background Information

Reinforcement learning (RL) is the sub-ﬁeld of machine learning concerned with decision making and simulation-based optimization. It studies how an agent can learn how to achieve goals in a complex, uncertain environment and is concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.

Transfer Learning (TL) is a recent development in the ﬁeld of Machine Learning that involves utilizing already trained agents to achieve faster and more eﬃcient results while tackling similar problems. The idea behind it is that, by leveraging the already trained agents, one can learn to solve similar problems by utilizing already learned information. TL in RL is an important topic to address at this time for three reasons. First, in recent years RL techniques have achieved notable successes in diﬃcult tasks which other machine learning techniques are either unable or ill-equipped to address. For instance, [Tes94], [ZD95],[CB98], [CAN09], [NMSI07], [RGHL09], [KS04] Second, classical machine learning techniques such as rule induction and classiﬁcation are suﬃciently mature that they may now easily be leveraged to assist with TL. Third, promising initial results show that not only are such transfer methods possible, but they can be very eﬀective at speeding up learning. There also has been increased interest in TL recently. The 2005 DARPA Transfer Learning program (DARPA, 2005) helped in providing exposure for RL techniques that use transfer. The 2005 NIPS workshop, “Inductive Transfer: 10 Years Later,” (Silver et al., 2005) had few RL-related transfer papers, the 2006 ICML workshop, “Structural Knowledge Transfer for Machine Learning,” (Banerjee et al., 2006) had many, and the 2008 AAAI workshop, “Transfer Learning for Complex Tasks,” (Taylor et al., 2008a) focused on RL.
### Our Approach

Our approach is to ﬁrst select 4 OpenAI Gym environment games that are suitable for giving insights into the eﬃcacy of our transfer learning. Some important considerations that we include in choosing: • We want some games that are similar (e.g. learning to walk) which will test the hypothesis that if we train our agent on one, it should be able to ﬁgure out a similar game quicker. • We want games of varying complexity which will test the hypothesis that if we ﬁrst train our agent on a complex game, the agent will be able to learn a simple game up quicker. After looking at diﬀerent models, we choose to use the following 4 models from OpenAI gym [BCP+16] for testing the eﬀectiveness of Transfer Learning in the domain of Reinforcement Learning:
• Hopping Robot
• 2D Walker 
• Half Cheetah
• Ant

We choose these 4 environments because, echoing the points made above, they are a similar type of game and because there is varying complexity (ant game is easier).
Next, we want to obtain control group data for each of the 4 games we chose. In particular, we want to ﬁnd how many games it normally takes for our agent to play a game perfectly using reinforcement learning. If a game cannot be played perfectly, then we want to use a score threshold provided by OpenAI gym and see how long it takes to reach that threshold.
For each of the 4 games, we will go through and repeat the following process: 1. We ﬁrst train our reinforcement learning agent on the selected game. 2. We transfer what we learn and, for each permutation of the other 3 games, have our agent play them. If we train on game A and our other 3 games are game B, C, and D then we would try playing BCD, BDC, CBD, CDB, DBC, and DCB. 3. For each of these permutations we measure how many games it takes for the agent to play perfectly (or achieve a certain score threshold)
4 Evaluating Success We want to see how well our agent can learn from the ﬁrst game and apply the learned result to each permutation of games. If we say that without transfer learning it normally takes ni games for our agent to play game i perfectly and we say it takes mi games for our agent that has ﬁrst learned oﬀ of a diﬀerent game to play perfectly, then our hope is that mi < ni holds for most if not all games. As mentioned in the approach section, in the case that a game cannot be played perfectly, we can use a score threshold, provided by OpenAI gym, and see how long it takes for our agent to reach that score.
Our results will chieﬂy be in the form of what percentage decrease in number of games did we have to play to perfect a game or reach a score threshold. We also want to ask the question of which of the 4 games is the best game to train an agent to play generally as well as which permutation of games is best. Intuitively we expect a complex game to be the best. Additionally, we want to be able to discover new insights as mentioned in the beginning of Section 3 such as which games are more "similar" to each other.

### References
[BCP+16] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. [CAN09] AdamCoates, PieterAbbeel, andAndrewY.Ng. Apprenticeshiplearningforhelicopter control. Commun. ACM, 52(7):97–105, July 2009. [CB98] Robert H. Crites and Andrew G. Barto. Elevator group control using multiple reinforcement learning agents. Mach. Learn., 33(2-3):235–262, December 1998. [KS04] Nate Kohl and Peter Stone. Machine learning for fast quadrupedal locomotion. In Proceedings of the 19th National Conference on Artiﬁcal Intelligence, AAAI’04, pages 611–616. AAAI Press, 2004. [NMSI07] YutakaNakamura, TakeshiMori, Masa-akiSato, andShinIshii. Reinforcementlearning for a biped robot based on a cpg-actor-critic method. Neural Netw., 20(6):723–735, August 2007.
2
[RGHL09] Martin Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange. Reinforcement learning for robot soccer. Auton. Robots, 27(1):55–73, July 2009. [Tes94] Gerald Tesauro. Td-gammon, a self-teaching backgammon program, achieves masterlevel play. Neural Comput., 6(2):215–219, March 1994. [ZD95] Wei Zhang and Thomas G. Dietterich. A reinforcement learning approach to job-shop scheduling. In Proceedings of the 14th International Joint Conference on Artiﬁcial Intelligence - Volume 2, IJCAI’95, pages 1114–1120, San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc.
